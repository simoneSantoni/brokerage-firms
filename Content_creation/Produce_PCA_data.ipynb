{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Only-change-this,-the-input-file\" data-toc-modified-id=\"Only-change-this,-the-input-file-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Only change this, the input file</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T16:35:15.325111Z",
     "start_time": "2020-02-03T16:35:15.299047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "if 'nlp' not in locals():\n",
    "#     nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Text Analysis (uncomment if running for first time)\n",
    "# ! wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# ! unzip mallet-2.0.8.zip\n",
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pathlib import *\n",
    "#current working directory\n",
    "current_dir = Path.cwd()\n",
    "#go up 1 level to the 1st parent directory\n",
    "Par1_dir = current_dir.parents[0]\n",
    "\n",
    "# Alert Function\n",
    "def allDone():\n",
    "    '''this function outputs a short audio when called. \n",
    "    Typically this is used to signal a task completion'''\n",
    "    \n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only change this, the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T16:35:16.222538Z",
     "start_time": "2020-02-03T16:35:16.219990Z"
    }
   },
   "outputs": [],
   "source": [
    "# What folder is it in, 'ai' or 'dl'\n",
    "ai_or_dl = 'ai'\n",
    "df_filename = 'globe_#ML_en_130101_190314_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T16:35:23.498129Z",
     "start_time": "2020-02-03T16:35:17.309850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ai_project/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_dir = Par1_dir /'AI Project'/'Python Env' / 'DataFrames'/ 'AI_datasets' / 'globe_ai_13_18'/ df_filename\n",
    "\n",
    "#import only the required columns from the DF\n",
    "df = pd.read_csv(df_data_dir, usecols=['tweet', 'date/time', 'search'])\n",
    "\n",
    "#find dataframe's search word, and remove quotations f present\n",
    "search_word = df.head(1).search.item().replace('\"', '').replace(\"'\",\"\")\n",
    "#drop unused columns\n",
    "df = df.loc[:, ['tweet', 'date/time']]\n",
    "\n",
    "df_list = []\n",
    "# #create a random Sample of 100k for each year & drop NAs\n",
    "for x in range(2013,2019):\n",
    "    globals()['df_%s' % x] = df.loc[(df['date/time']>=str(x)+'-01-01 00:00:00+00:00') & (df['date/time']<str(x+1)+'-01-01 00:00:00+00:00'),: ].copy()\n",
    "    n = min(100000, len(globals()['df_%s' % x]))\n",
    "    globals()['df_%s' % x] = globals()['df_%s' % x].sample(n=n, random_state=11).dropna(subset=['tweet'])\n",
    "    df_list.append(globals()['df_%s' % x])\n",
    "    del globals()['df_%s' % x]\n",
    "\n",
    "#delete the big DF\n",
    "del df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T16:35:23.573225Z",
     "start_time": "2020-02-03T16:35:23.499959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#ML'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(df_list)\n",
    "search_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T16:56:18.196455Z",
     "start_time": "2020-02-03T16:35:23.575676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#NLTK english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#extend the list with a peronal list of stopwords\n",
    "stop_words.extend(['from', 'need','thank','thing','something', 'see', 'say', 'well','people', 'change', 'com',\\\n",
    "                   'go', 'put', 'give','twitter','pic', \\\n",
    "                   'subject', 're', 'edu', 'could', 'be', 'make', 'not', 'make','find','let','may','see', 'would',\\\n",
    "                   'come', 'sure', 'ever', 'tell', 'use', 'not', 'doing', 'be', 'get','want'])\n",
    "#extend the search word\n",
    "stop_words.extend(['artificial intelligence', '#ai', '#ml', '#nlp', 'analytics', 'data mining',\n",
    "                  'deep mining', 'machine learning', 'natural language processing', 'neural network'\n",
    "                  'pattern recognition'])\n",
    "\n",
    "### Tokenize words and Clean-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "###Remove Stopwords, Make Bigrams and Lemmatize\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(bigram_mod, texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def lda_preprocessing(df):\n",
    "    ### Remove emails, newline characters, and links\n",
    "    # Convert to list\n",
    "    data = df.tweet.values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', str(tweet)) for tweet in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', tweet) for tweet in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", tweet) for tweet in data]\n",
    "\n",
    "    #Remove links\n",
    "    data = [re.sub(r\"http\\S+\", \"\", tweet) for tweet in data]\n",
    "\n",
    "    #make lower case\n",
    "    data = [tweet.lower() for tweet in data]\n",
    "    \n",
    "    # Tokenize words and Clean-up text\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    ###Creating Bigram and Trigram Models\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(bigram_mod, data_words_nostops)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ','ADV', 'VERB'])\n",
    "    data_lemmatized = remove_stopwords(data_lemmatized)\n",
    "\n",
    "    ###Create the Dictionary and Corpus needed for Topic ModelingÂ¶\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        \n",
    "    return data_lemmatized, corpus, id2word\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "#     df = df_list[1]\n",
    "\n",
    "data_lemmatized, CORPUS, id2word = lda_preprocessing(df)\n",
    "\n",
    "#get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#set the number of workers to the number of cores available\n",
    "workers = cpu_count() \n",
    "\n",
    "# Set number of topics\n",
    "NUM_TOPICS = 20\n",
    "\n",
    "ldamallet = LdaMallet(MALLET_PATH, corpus=CORPUS, num_topics=NUM_TOPICS, id2word=id2word,\n",
    "                                  random_seed =11,workers=workers)\n",
    "\n",
    "# Transform model weights from MALLET to GENSIM\n",
    "lda_model = malletmodel2ldamodel(ldamallet)\n",
    "\n",
    "# Save Topic meanings\n",
    "topic_content = pd.DataFrame(ldamallet.print_topics(num_topics=NUM_TOPICS, num_words=100))\n",
    "topic_content = topic_content.rename(columns={0:'topic_n', 1:'content'})\n",
    "\n",
    "filename = 'topic_content_' + search_word + '.csv'\n",
    "foldername = 'pca_data'\n",
    "save_dir = Par1_dir /'AI Project'/'Avery_output' / foldername / ai_or_dl / filename\n",
    "topic_content.to_csv(save_dir, index=False)\n",
    "\n",
    "############################\n",
    "# Get doc-topic probability#\n",
    "############################\n",
    "\n",
    "# get transformed corpus as per the LDA model\n",
    "TRANSF_CORPUS = lda_model.get_document_topics(CORPUS)\n",
    "\n",
    "# rearrange data on document-topic pairs probabilities\n",
    "DOC_TOPIC_M = []\n",
    "\n",
    "for id, doc in enumerate(TRANSF_CORPUS):\n",
    "    for topic in np.arange(0, 20, 1):\n",
    "        topic_n = doc[topic][0]\n",
    "        topic_prob = doc[topic][1] \n",
    "        DOC_TOPIC_M.append([id, topic, topic_prob])\n",
    "\n",
    "DF = pd.DataFrame(DOC_TOPIC_M)\n",
    "\n",
    "# rename columns\n",
    "OLD_NAMES = [0, 1, 2]\n",
    "NEW_NAMES = ['doc_id', 'topic_n', 'prob']\n",
    "COLS = dict(zip(OLD_NAMES, NEW_NAMES))\n",
    "DF.rename(columns=COLS, inplace=True)\n",
    "\n",
    "DF1 = DF.copy() # Checkpoint\n",
    "DF1 = DF1.pivot_table(index='doc_id', columns='topic_n', values='prob', aggfunc=np.mean)\n",
    "DF1 = DF1.reset_index()\n",
    "\n",
    "# Keep only dominant topic for vis\n",
    "GR = DF.groupby('doc_id')\n",
    "DF.loc[:, 'max'] = GR['prob'].transform(np.max)\n",
    "DF.loc[:, 'first_topic'] = 0\n",
    "DF.loc[DF['prob'] == DF['max'], 'first_topic'] = 1\n",
    "FIRST_TOPIC = DF.loc[DF['first_topic'] == 1]\n",
    "\n",
    "# Record Dates\n",
    "dates = pd.DataFrame()\n",
    "dates['date'] = df['date/time']\n",
    "dates['year'] = dates['date'].str.split('-', expand=True)[0]\n",
    "dates = dates.reset_index(drop=True) \n",
    "dates = dates.reset_index()\n",
    "dates = dates.rename(columns={'index':'doc_id'})\n",
    "\n",
    "# Merge to keep the First order\n",
    "DF2 = pd.merge(DF1, FIRST_TOPIC, how='left', on='doc_id')\n",
    "DF2 = pd.merge(DF2, dates, how='left', on='doc_id') \n",
    "\n",
    "# write data to file\n",
    "filename = search_word + '.csv'\n",
    "foldername = 'pca_data'\n",
    "save_dir = Par1_dir /'AI Project'/'Avery_output' / foldername / ai_or_dl / filename\n",
    "DF2.to_csv(save_dir, index=False)\n",
    "\n",
    "def pca_data(df):\n",
    "\n",
    "    df1 = df.loc[:, '0':'19']\n",
    "    X = df1.to_numpy()\n",
    "\n",
    "    df2 = df.drop(columns=df1.columns)\n",
    "    df2 = df2[['topic_n', 'prob', 'date', 'year']]\n",
    "    scaler = StandardScaler() # instantiate\n",
    "    scaler.fit(X) # compute the mean and standard which will be used in the next command\n",
    "    X_scaled = scaler.transform(X)\n",
    "    # fit and transform can be applied together and I leave that for simple exercise\n",
    "    # we can check the minimum and maximum of the scaled features which we expect to be 0 and 1\n",
    "    # print (\"after scaling minimum\", X_scaled.min(axis=0))\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_scaled)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    vdf = pd.DataFrame()\n",
    "    vdf['x'] = X_pca[:,0]\n",
    "    vdf['y'] = X_pca[:,1]\n",
    "    # vdf = vdf.reset_index()\n",
    "    # vdf = vdf.rename(columns={'index':'doc_id'})\n",
    "    vdf = pd.merge(vdf, df2, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    return vdf\n",
    "\n",
    "DF3 = pd.read_csv(save_dir)\n",
    "vdf = pca_data(DF3)\n",
    "filename = 'vis_ready_' + search_word + '.csv'\n",
    "foldername = 'pca_data'\n",
    "save_dir = Par1_dir /'AI Project'/'Avery_output' / foldername / ai_or_dl / filename\n",
    "vdf.to_csv(save_dir, index=False)\n",
    "\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

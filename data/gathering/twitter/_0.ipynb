{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Monthly Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the getoldtweet package to scrape monthly tweets from twitters search results page. also it contains sme post processing options in the lower cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T18:48:39.514129Z",
     "start_time": "2019-07-18T18:48:39.509620Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Monthly Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct each group to be scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T18:48:40.173845Z",
     "start_time": "2019-07-18T18:48:40.169281Z"
    }
   },
   "outputs": [],
   "source": [
    "#Artificial Intelligence graph terms \n",
    "AI_graph = ['#AI', '#ML', '#NLP', 'Artificial Intelligence','Deep Learning',\n",
    "            '\"Machine Learning\"', 'Natural Language Processing','Neural Network']\n",
    "#Distributed Ledger graph terms\n",
    "DL_graph = ['Bitcoin', 'Blockchain','Ethereum','distributed ledger','smart contract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T18:48:41.597401Z",
     "start_time": "2019-07-18T18:48:41.594252Z"
    }
   },
   "outputs": [],
   "source": [
    "def allDone():\n",
    "    '''this function outputs a short funny audio when called. \n",
    "    Typically this is used to signal a task completion'''\n",
    "    \n",
    "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "\n",
    "def update_tweet_csv(path,DF,start,end,delta,Verbose=True):\n",
    "    '''This function saves the results of the scrape to the disk. it is meant to be passed \n",
    "    within a loop and append data being scraped with each loop to the DF stored on the disk. typically the loop \n",
    "    runs daily scrapes for the period of a month'''\n",
    "    \n",
    "    #if the scrape was successful and the file doesnt exist then create a file and save the DF as a csv\n",
    "    if len(DF)>0 and os.path.isfile(path) == False:            \n",
    "        DF.to_csv(path, index=False) \n",
    "        #start and end parameters dont need editing since scrape was successful\n",
    "        start, end = start, end\n",
    "        #print date scraped, time of scrape, and number of daily tweets scraped \n",
    "        if Verbose==True:\n",
    "            print(since,\" // \",datetime.now(),\" / \", round(len(DF)),\" tweets/day\")\n",
    "    \n",
    "    #if the scrape is successful and file name exists, then append to it\n",
    "    elif len(DF)>0 and os.path.isfile(path) == True:\n",
    "        #open the csv of the month being scraped\n",
    "        globe = pd.read_csv(path)\n",
    "        #append the day scraped \n",
    "        globe = globe.append(DF)\n",
    "        #save new DF to the csv\n",
    "        globe.to_csv(path, index=False)\n",
    "        start, end = start, end\n",
    "        if Verbose==True:\n",
    "            print(since,\" // \",datetime.now(),\" // \", round(len(DF)),\" tweets/day \",len(globe))\n",
    "    #If twitter data was not reached due to any interruptions/block wait then try that day again\n",
    "    elif len(DF)==0:\n",
    "        if Verbose==True:\n",
    "            print(since,\" // \",datetime.now(),\" // \", round(len(DF)),\" tweets/day **\")\n",
    "        #adjust the start and end dates to retry scraping this day\n",
    "        start -= delta\n",
    "        end   -= delta\n",
    "        time.sleep(60)\n",
    "        \n",
    "    return start, end\n",
    "\n",
    "def tweets_to_df(tweet):\n",
    "    '''this function saves the results of the twitter scrapes into lists then creates a DF out of them.\n",
    "    this is needed to extract info from the  getoldtweets3 generator object'''\n",
    "    #initialize lists\n",
    "    text, date, hashtag, username, link, keyword, ID = [], [], [], [], [], [], []\n",
    "    \n",
    "    #add content to lists using GOT3 \"tweet\" generator object\n",
    "    for tweets in tweet:\n",
    "        text.append(str(tweets.text))\n",
    "        date.append(str(tweets.date))\n",
    "        hashtag.append(str(tweets.hashtags))\n",
    "        username.append(str(tweets.username))\n",
    "        link.append(tweets.permalink)\n",
    "        keyword.append(word)\n",
    "        ID.append(tweets.id)\n",
    "\n",
    "    #compile content into a DF\n",
    "    DF = pd.DataFrame({'tweet':text, 'date/time':date, 'hashtags':hashtag, 'user':username, 'links':link,\n",
    "                       'search':keyword,'tweet_id':ID})\n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  why twitter has limitations and why you should download in daily intervals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"The issue here is **Min_position** and **Has_more_items** flags. Twitter's legacy timeline caching system **Haplo** has its limitations. So when you start downloading millions of tweets, it runs out of memory and sometimes returns has_more_items as false. You can read about how twitter cache works in here\n",
    "\n",
    "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html  \"\n",
    "\n",
    "source: https://github.com/Mottl/GetOldTweets3/issues/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Monthly Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info relating to the steps to be followed below:\n",
    "- set the start and end date to be scraped\n",
    "- scrapes are ran in daily intervals because that is the smallest interval allowed by twitter.(e.g. since the results are scraped in descending chronological order if a scrape is ran over a week and gets interrupted,due to hash issues, days worth of data can be lost, however if a single day's scrape gets interrupted then only hours are lost. this saves the user the hassle of rechecking for missing days and rescraping.)\n",
    "- if the user doesnt want to see process updates set verbose==False in the update_tweet_csv function\n",
    "\n",
    "Background info:\n",
    "- the typical speed of GOT3 is roughly 2.5 million tweets/day\n",
    "- scraping a month worth's data, using the lists above(AI_graph and DL_graph), takes a full day\n",
    "- using a different proxy for each request(20 tweets) using services like crawlera reduces scraping speed by 5.5 times.\n",
    "- it is recommended to use a diffferent IP address for each day of scraping or scraping gets blocked by twitter repeatedly 10  cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-18T19:42:20.449Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globe_Bitcoin_2019-03.csv \n",
      "start:  2019-07-23 22:53:29.471256\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n",
      "An error occured during an HTTP request: [Errno 104] Connection reset by peer\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2019-03-21%20until%3A2019-03-22&src=typd\n",
      "error occured at  2019-03-21\n"
     ]
    }
   ],
   "source": [
    "for word in AI_graph+DL_graph:\n",
    "    \n",
    "    delta = timedelta(days = 1)            #set scrape range (e.g. number of days, ,weeks, months)\n",
    "    start = datetime(2019,7,1) - delta     #set first day of scrape \n",
    "    x = start + 2*delta                    # x is the element used in the while loop indicating the current start date being scraped\n",
    "    stop_point = datetime(2019,8,1)        #set final day of scrape, this is not inclusive\n",
    "    \n",
    "    data_dir = os.getcwd() + '/twitter_data_2019/'\n",
    "    file_name = 'globe_' + word + \"_\" + (start+delta).strftime('%Y-%m') + '.csv'\n",
    "    print(file_name, '\\nstart: ', datetime.now())\n",
    "\n",
    "    while x < stop_point:\n",
    "        try:    \n",
    "            start += delta\n",
    "            end = start + delta\n",
    "            since = (start).strftime(\"%Y-%m-%d\")\n",
    "            until = (end).strftime(\"%Y-%m-%d\")\n",
    "            x = end\n",
    "\n",
    "\n",
    "            #Get tweets by query search\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word).setSince(since).setUntil(until)\n",
    "            tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "            \n",
    "            #store the data as a DF\n",
    "            DF = tweets_to_df(tweet)\n",
    "            \n",
    "            #save the daily scrape to csv on disk and update start & end accordingly\n",
    "            path = data_dir + file_name\n",
    "            start, end = update_tweet_csv(path,DF,start,end,delta,Verbose=True)\n",
    "            #minimize memory retention\n",
    "            del [DF, tweet, tweetCriteria]\n",
    "            gc.collect()\n",
    "        \n",
    "        #in case of an error occuring mid a scrape cycle, wait then repeat the cycle    \n",
    "        except:\n",
    "            print('error occured at ', since, datetime.now())\n",
    "            #maintian same date and dont save the data\n",
    "            start -= delta\n",
    "            end   -= delta\n",
    "            #wait a while before trying again\n",
    "            time.sleep(120)\n",
    "       \n",
    "    #audio signal when each each phrase/month finishes\n",
    "    allDone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Check Continuity of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As mentioned above, due to hash issues and others, twitter sometimes limits the results returned in a search. to detect the missing data use the cell below and discover the  number of hours and dates missing. \n",
    "\n",
    "info  to be filled:\n",
    "- the \"filename\" should be changed to the psth of the desired csv\n",
    "- change the range of dates to be searched for missing data by changing b(end date) and a(start date)\n",
    "- set the min_hrs parameter which is used to show the days with more than a certain number of hours missing(e.g. min_hrs=2 then only dates with more than 2 hrs missing will be printed)\n",
    "\n",
    "results:\n",
    "- percent of hours missed (typically a DF will have <3% of missing data if scrape is done in daily intervals as is recommended)\n",
    "- a list showing how many days have how many hours missing (# of hrs, num of days) e.g.[(1, 4), (2, 2),..., (23, 5), (24, 2)]\n",
    "- the number of days with more than min_hrs missing\n",
    "- date of each day with more than min_hrs missing (this list can later be used to rescrape dates with significant number of hrs/day missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globe_Bitcoin_130101_170121_6.csv 2019-05-14 21:43:49.747945\n",
      "Missing:  2.160702228224173 % \n",
      " [(1, 4), (2, 2), (3, 2), (4, 2), (6, 1), (7, 2), (8, 2), (9, 3), (10, 4), (11, 4), (13, 1), (14, 3), (15, 2), (16, 2), (18, 2), (19, 5), (20, 4), (21, 2), (22, 3), (23, 5), (24, 2)]\n",
      " # Days:  53 \n",
      " Ranges sizes:  [datetime.date(2014, 5, 6), datetime.date(2014, 5, 9), datetime.date(2015, 10, 24), datetime.date(2016, 3, 6), datetime.date(2016, 7, 3), datetime.date(2016, 7, 9), datetime.date(2016, 7, 21), datetime.date(2016, 7, 29), datetime.date(2016, 7, 31), datetime.date(2016, 8, 3), datetime.date(2016, 8, 4), datetime.date(2016, 8, 6), datetime.date(2016, 8, 7), datetime.date(2016, 8, 8), datetime.date(2016, 8, 9), datetime.date(2016, 8, 10), datetime.date(2016, 8, 11), datetime.date(2016, 8, 12), datetime.date(2016, 8, 13), datetime.date(2016, 8, 14), datetime.date(2016, 8, 16), datetime.date(2016, 8, 17), datetime.date(2016, 8, 18), datetime.date(2016, 8, 19), datetime.date(2016, 8, 20), datetime.date(2016, 8, 21), datetime.date(2016, 9, 13), datetime.date(2016, 9, 14), datetime.date(2016, 9, 15), datetime.date(2016, 9, 23), datetime.date(2016, 9, 24), datetime.date(2016, 9, 25), datetime.date(2016, 9, 26), datetime.date(2016, 9, 27), datetime.date(2016, 9, 28), datetime.date(2016, 9, 29), datetime.date(2016, 9, 30), datetime.date(2016, 10, 1), datetime.date(2016, 10, 2), datetime.date(2016, 10, 3), datetime.date(2016, 10, 4), datetime.date(2016, 10, 5), datetime.date(2016, 10, 15), datetime.date(2016, 10, 16), datetime.date(2016, 10, 17), datetime.date(2016, 10, 18), datetime.date(2016, 10, 19), datetime.date(2016, 10, 20), datetime.date(2016, 10, 21), datetime.date(2016, 10, 22), datetime.date(2016, 10, 23), datetime.date(2017, 1, 5), datetime.date(2017, 1, 9)]\n"
     ]
    }
   ],
   "source": [
    "#### Check if data is continous\n",
    "filename = 'globe_Bitcoin_130101_170121_6.csv'\n",
    "\n",
    "print(filename, datetime.now())\n",
    "\n",
    "# get hours scraped, for days change 13 to 10\n",
    "actual = set([datetime.strptime(date_str[:13],\"%Y-%m-%d %H\")  for date_str in  pd.read_csv(filename)['date/time']])\n",
    "\n",
    "# generate all possible hours in date range\n",
    "b = datetime(2017,1,21)\n",
    "a = datetime(2013,1,1)\n",
    "numhrs = 24*((b.date()-a.date()).days)\n",
    "dateList = []\n",
    "for x in range(0, numhrs+2):\n",
    "    dateList.append((a - timedelta(hours = x)))\n",
    "\n",
    "#the list incomplete/missing dates\n",
    "min_hrs = 1                                                     #the minumum number hours needed to display date\n",
    "hours_missed = sorted(set(dateLis) - actual)                    #all missing hours\n",
    "counter = Counter([date.date() for date in hours_missed])       #count hours missed per day\n",
    "sort = sorted(counter.items())                                  #sort in chronological order\n",
    "dates_missed = [date[0] for date in sort if date[1]>min_hrs]    #keep dates with more than 2 hours missing\n",
    "\n",
    "#calculate the total number of hours missed as a percentage\n",
    "summary = Counter([date[1] for date in sort])                   \n",
    "summary = sorted(summary.items())                               \n",
    "total_missed_hours = sum([x[0]*x[1] for x in summary])\n",
    "print('Missing: ', total_missed_hours*100/numhrs, \"%\",\"\\n\",summary)\n",
    "\n",
    "# create since and until to search twitter for those missing date ranges\n",
    "since_missing = dates_missed\n",
    "until_missing = [dm + timedelta(days=1) for dm in dates_missed]\n",
    "\n",
    "print(\" # Days: \", len(dates_missed),\"\\n\",\n",
    "     \"Ranges sizes: \", since_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rescrape missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "this cell is used to rescrape days with significant missing hours. this cell is optional and can be avoided by simply scraping 1 day at a time as reccomended above. it was only created to ammend scrapes initially done in larger intervals (1 week scrapes). however since twitter's smallest date interval is a date then by setting the scrape to 1 day intervals the highest accracy will be acheived from the start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:  globe_Bitcoin_130101_170121_5_missing.csv 2019-05-13 23:03:51.866060\n",
      "An error occured during an HTTP request: HTTP Error 503: Service Temporarily Unavailable\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2016-08-09%20until%3A2016-08-10&src=typd\n",
      "ERROR:  2016-08-09\n",
      "2016-08-10 --> 2016-08-11  //  2019-05-13 23:13:44.042894  //  9338.0 rows/days\n",
      "An error occured during an HTTP request: HTTP Error 503: Service Temporarily Unavailable\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2016-08-11%20until%3A2016-08-12&src=typd\n",
      "ERROR:  2016-08-11\n",
      "2016-08-12 --> 2016-08-13  //  2019-05-13 23:20:46.029110  //  8622.0 rows/days\n",
      "2016-08-13 --> 2016-08-14  //  2019-05-13 23:24:37.337142  //  8171.0 rows/days\n",
      "2016-08-14 --> 2016-08-15  //  2019-05-13 23:25:26.047601  //  1609.0 rows/days\n",
      "2016-08-16 --> 2016-08-17  //  2019-05-13 23:26:58.859637  //  3061.0 rows/days\n",
      "2016-08-17 --> 2016-08-18  //  2019-05-13 23:27:53.489701  //  1816.0 rows/days\n",
      "2016-08-18 --> 2016-08-19  //  2019-05-13 23:30:50.469625  //  5865.0 rows/days\n",
      "2016-08-19 --> 2016-08-20  //  2019-05-13 23:36:36.151479  //  11379.0 rows/days\n",
      "2016-08-20 --> 2016-08-21  //  2019-05-13 23:38:18.567312  //  3493.0 rows/days\n",
      "2016-08-21 --> 2016-08-22  //  2019-05-13 23:42:18.191806  //  8309.0 rows/days\n",
      "2016-09-13 --> 2016-09-14  //  2019-05-13 23:43:29.961258  //  2272.0 rows/days\n",
      "2016-09-14 --> 2016-09-15  //  2019-05-13 23:49:27.926530  //  12327.0 rows/days\n",
      "2016-09-15 --> 2016-09-16  //  2019-05-13 23:49:32.431811  //  9.0 rows/days\n",
      "2016-09-23 --> 2016-09-24  //  2019-05-13 23:50:01.623738  //  868.0 rows/days\n",
      "2016-09-24 --> 2016-09-25  //  2019-05-13 23:51:57.643066  //  3494.0 rows/days\n",
      "2016-09-25 --> 2016-09-26  //  2019-05-13 23:56:11.069297  //  8119.0 rows/days\n",
      "2016-09-26 --> 2016-09-27  //  2019-05-14 00:01:03.793155  //  9719.0 rows/days\n",
      "2016-09-27 --> 2016-09-28  //  2019-05-14 00:05:52.177459  //  9444.0 rows/days\n",
      "2016-09-28 --> 2016-09-29  //  2019-05-14 00:09:58.513244  //  7998.0 rows/days\n",
      "2016-09-29 --> 2016-09-30  //  2019-05-14 00:11:02.528383  //  2001.0 rows/days\n",
      "2016-09-30 --> 2016-10-01  //  2019-05-14 00:15:41.658152  //  8926.0 rows/days\n",
      "2016-10-01 --> 2016-10-02  //  2019-05-14 00:15:46.821455  //  11.0 rows/days\n",
      "2016-10-02 --> 2016-10-03  //  2019-05-14 00:16:50.061835  //  1962.0 rows/days\n",
      "2016-10-03 --> 2016-10-04  //  2019-05-14 00:20:10.547642  //  6263.0 rows/days\n",
      "2016-10-04 --> 2016-10-05  //  2019-05-14 00:26:13.888698  //  12065.0 rows/days\n",
      "2016-10-05 --> 2016-10-06  //  2019-05-14 00:31:07.356171  //  9641.0 rows/days\n",
      "2016-10-15 --> 2016-10-16  //  2019-05-14 00:32:08.902420  //  1813.0 rows/days\n",
      "2016-10-16 --> 2016-10-17  //  2019-05-14 00:33:00.213049  //  1466.0 rows/days\n",
      "2016-10-17 --> 2016-10-18  //  2019-05-14 00:33:49.733189  //  1430.0 rows/days\n",
      "An error occured during an HTTP request: HTTP Error 503: Service Temporarily Unavailable\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2016-10-18%20until%3A2016-10-19&src=typd\n",
      "ERROR:  2016-10-18\n",
      "2016-10-19 --> 2016-10-20  //  2019-05-14 00:34:58.033038  //  357.0 rows/days\n",
      "2016-10-20 --> 2016-10-21  //  2019-05-14 00:35:04.024877  //  8.0 rows/days\n",
      "2016-10-21 --> 2016-10-22  //  2019-05-14 00:36:24.029684  //  2459.0 rows/days\n",
      "2016-10-22 --> 2016-10-23  //  2019-05-14 00:37:22.801745  //  1789.0 rows/days\n",
      "2016-10-23 --> 2016-10-24  //  2019-05-14 00:41:36.210158  //  8536.0 rows/days\n",
      "An error occured during an HTTP request: HTTP Error 503: Service Temporarily Unavailable\n",
      "Try to open in browser: https://twitter.com/search?q=Bitcoin%20since%3A2017-01-05%20until%3A2017-01-06&src=typd\n",
      "ERROR:  2017-01-05\n",
      "2017-01-09 --> 2017-01-10  //  2019-05-14 00:45:32.897698  //  743.0 rows/days\n"
     ]
    }
   ],
   "source": [
    "# import urllib3\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore', category=ResourceWarning)\n",
    "\n",
    "#Amended search for missing dates\n",
    "filename = 'globe_' + AI_graph[0] + '_130101_170121_5_missing.csv'\n",
    "print(\"start: \", filename, datetime.now())\n",
    "\n",
    "word = AI_graph[0]   \n",
    "for i in range(len(since_missing)):\n",
    "    if (until_missing[i] - since_missing[i]).days <= 0:\n",
    "        continue\n",
    "        \n",
    "    since = (since_missing[i]).strftime(\"%Y-%m-%d\")\n",
    "    until = (until_missing[i]).strftime(\"%Y-%m-%d\")\n",
    "    text, date, hashtag, username, link, keyword, ID = [], [], [], [], [], [], []\n",
    "\n",
    "    try:\n",
    "        #Get tweets by query search\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word).setSince(since).setUntil(until)\n",
    "        tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    except:\n",
    "        print('ERROR: ', since)\n",
    "        time.sleep(15)\n",
    "        continue\n",
    "\n",
    "    #add content to lists\n",
    "    for tweets in tweet:\n",
    "        text.append(str(tweets.text))\n",
    "        date.append(str(tweets.date))\n",
    "        hashtag.append(str(tweets.hashtags))\n",
    "        username.append(str(tweets.username))\n",
    "        link.append(tweets.permalink)\n",
    "        keyword.append(word)\n",
    "        ID.append(tweets.id)\n",
    "\n",
    "    #compile content into a DF\n",
    "    DF = pd.DataFrame({'tweet':text, 'date/time':date, 'hashtags':hashtag, 'user':username, 'links':link,\n",
    "                       'search':keyword,'tweet_id':ID})\n",
    "\n",
    "\n",
    "    if len(DF)>0 and os.path.isfile(filename) == False:            \n",
    "        DF.to_csv(filename, index=False) \n",
    "        print(since,\"-->\",until,\" // \",datetime.now(),\" / \", len(DF)/(until_missing[i] - since_missing[i]).days,\"rows/days\")\n",
    "        del [DF, text, date, hashtag, username, link, keyword, ID, tweet, tweetCriteria ]\n",
    "        gc.collect()\n",
    "        continue\n",
    "    elif len(DF)>0 and os.path.isfile(filename) == True:\n",
    "        globe = pd.read_csv(filename)\n",
    "        globe = globe.append(DF)\n",
    "        globe.to_csv(filename, index=False)\n",
    "        print(since,\"-->\",until,\" // \",datetime.now(),\" // \", (len(DF))/(until_missing[i] - since_missing[i]).days,\"rows/days\")\n",
    "        del [globe, DF, text, date, hashtag, username, link, keyword, ID, tweet, tweetCriteria ]\n",
    "        gc.collect()\n",
    "        continue     \n",
    "    else:\n",
    "        print(since,\" // \",datetime.now(),\" // \",\" 0 rows\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

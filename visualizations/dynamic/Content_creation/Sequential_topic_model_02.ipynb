{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-and-directory\" data-toc-modified-id=\"Import-and-directory-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import and directory</a></span></li><li><span><a href=\"#Sequential-Topic-Modelling\" data-toc-modified-id=\"Sequential-Topic-Modelling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sequential Topic Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-for-model\" data-toc-modified-id=\"Import-for-model-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import for model</a></span></li><li><span><a href=\"#Stop-Words\" data-toc-modified-id=\"Stop-Words-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Stop Words</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Sequential-modelling-part\" data-toc-modified-id=\"Sequential-modelling-part-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Sequential modelling part</a></span></li><li><span><a href=\"#Get-document-topic-dominant-probability\" data-toc-modified-id=\"Get-document-topic-dominant-probability-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Get document-topic dominant probability</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T04:48:05.488553Z",
     "start_time": "2020-01-12T04:48:05.485960Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T04:53:20.941951Z",
     "start_time": "2020-01-12T04:53:20.937833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/simone/data/ra/avery/google_drive')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import *\n",
    "#current working directory\n",
    "# current_dir = Path.cwd()\n",
    "# Rename to suit simone folders\n",
    "current_dir = Path('/media/simone/data/ra/avery/google_drive/ai')\n",
    "# /media/simone/data/ra/avery/google_drive/ai/Avery_output/AI_sequential_topic/\n",
    "\n",
    "#go up 1 level to the 1st parent directory\n",
    "parent_dir = current_dir.parents[0]\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T04:53:28.834007Z",
     "start_time": "2020-01-12T04:53:28.829797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/simone/data/ra/avery/google_drive/ai/Avery_output/AI_sequential_topic/AI_topic_company_10k.csv')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filename = 'AI_topic_company_10k.csv'\n",
    "data_dir = parent_dir / 'ai' / 'Avery_output' / 'AI_sequential_topic' / data_filename\n",
    "\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T01:45:16.286857Z",
     "start_time": "2019-12-15T01:45:16.283927Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "def allDone():\n",
    "    '''this function outputs a short audio when called. \n",
    "    Typically this is used to signal a task completion'''\n",
    "    \n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "if 'nlp' not in locals():\n",
    "    nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "#     nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "import gc\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Text Analysis (uncomment if running for first time)\n",
    "# ! wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# ! unzip mallet-2.0.8.zip\n",
    "# MALLET_PATH = 'mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#NLTK english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#extend the list with a peronal list of stopwords\n",
    "stop_words.extend(['from', 'need','thank','thing','something', 'see', 'say', 'well','people', 'change', 'com',\\\n",
    "                   'go', 'put', 'give','twitter','pic', \\\n",
    "                   'subject', 're', 'edu', 'could', 'be', 'make', 'not', 'make','find','let','may','see', 'would',\\\n",
    "                   'come', 'sure', 'ever', 'tell', 'use', 'not', 'doing', 'be', 'get','want'])\n",
    "#extend the search word\n",
    "stop_words.extend(['artificial intelligence', '#ai', '#ml', '#nlp', 'analytics', 'data mining',\n",
    "                  'deep mining', 'machine learning', 'natural language processing', 'neural network'\n",
    "                  'pattern recognition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenize words and Clean-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "###Remove Stopwords, Make Bigrams and Lemmatize\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(bigram_mod, texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def lda_preprocessing(df):\n",
    "    ### Remove emails, newline characters, and links\n",
    "    # Convert to list\n",
    "    data = df.tweets.values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', str(tweet)) for tweet in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', tweet) for tweet in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", tweet) for tweet in data]\n",
    "\n",
    "    #Remove links\n",
    "    data = [re.sub(r\"http\\S+\", \"\", tweet) for tweet in data]\n",
    "\n",
    "    #make lower case\n",
    "    data = [tweet.lower() for tweet in data]\n",
    "    \n",
    "    # Tokenize words and Clean-up text\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    ###Creating Bigram and Trigram Models\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(bigram_mod, data_words_nostops)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    # Remove adj and adv just for this topic model\n",
    "    # data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ','ADV', 'VERB'])\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB'])\n",
    "    data_lemmatized = remove_stopwords(data_lemmatized)\n",
    "\n",
    "    ###Create the Dictionary and Corpus needed for Topic Modeling¶\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        \n",
    "    return data_lemmatized, corpus, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential modelling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Packages\n",
    "from gensim.models import LdaSeqModel\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "# Import df\n",
    "df = pd.read_csv(data_dir)\n",
    "\n",
    "data_lemmatized, corpus, id2word = lda_preprocessing(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avery/anaconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py:1474: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n",
      "/home/avery/anaconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py:293: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  convergence = np.fabs((bound - old_bound) / old_bound)\n",
      "/home/avery/anaconda3/lib/python3.7/site-packages/gensim/models/ldaseqmodel.py:1474: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  converged = np.fabs((lhood_old - lhood) / (lhood_old * total))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change slice based on how many each year \n",
    "\n",
    "### For 100k dataset use this\n",
    "# topic_model = LdaSeqModel(corpus=corpus, id2word=id2word,\n",
    "#                           time_slice=[100000, 100000, 100000, 100000, 100000, 100000, 75179],\n",
    "#                           num_topics=20)\n",
    "\n",
    "\n",
    "# For 10k dataset use this\n",
    "topic_model = LdaSeqModel(corpus=corpus, id2word=id2word,\n",
    "                          time_slice=[10000, 10000, 10000],\n",
    "                          num_topics=20)\n",
    "\n",
    "\n",
    "# Get the document topic probability relationship\n",
    "num_topics = 20\n",
    "matrix = []\n",
    "for x in range(len(corpus)):\n",
    "    matrix.append(topic_model.doc_topics(x))\n",
    "\n",
    "    \n",
    "df_ref = pd.DataFrame(matrix)\n",
    "\n",
    "df_dom = pd.DataFrame()\n",
    "\n",
    "# ref: https://thispointer.com/pandas-find-maximum-values-position-in-columns-or-rows-of-a-dataframe/\n",
    "#Get Column names of Maximum value in every row\n",
    "df_dom['topic_n'] = df_ref.idxmax(axis=1)\n",
    "df_dom['prob'] = df_ref.max(axis=1)\n",
    "\n",
    "#Merge with df100k and save\n",
    "\n",
    "df = df.merge(df_dom, left_index=True, right_index=True)\n",
    "\n",
    "OUTPUT = parent_dir/'ai'/'Avery_output'/'AI_sequential_topic'\n",
    "outname ='AI_topic_company_10k_domtopic.csv'\n",
    "\n",
    "\n",
    "df.to_csv(os.path.join(str(OUTPUT),outname), index = False)\n",
    "\n",
    "\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T02:11:55.021847Z",
     "start_time": "2019-12-15T02:11:53.514918Z"
    }
   },
   "source": [
    "## Get document-topic dominant probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document topic probability relationship\n",
    "num_topics = 20\n",
    "matrix = []\n",
    "for x in range(len(corpus)):\n",
    "    matrix.append(topic_model.doc_topics(x))\n",
    "\n",
    "    \n",
    "df_ref = pd.DataFrame(matrix)\n",
    "\n",
    "df_dom = pd.DataFrame()\n",
    "\n",
    "# ref: https://thispointer.com/pandas-find-maximum-values-position-in-columns-or-rows-of-a-dataframe/\n",
    "#Get Column names of Maximum value in every row\n",
    "df_dom['topic_n'] = df_ref.idxmax(axis=1)\n",
    "df_dom['prob'] = df_ref.max(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with df100k and save\n",
    "\n",
    "df = df.merge(df_dom, left_index=True, right_index=True)\n",
    "\n",
    "OUTPUT = parent_dir/'ai'/'Avery_output'/'AI_sequential_topic'\n",
    "outname ='AI_topic_company_10k_domtopic.csv'\n",
    "\n",
    "\n",
    "df.to_csv(os.path.join(str(OUTPUT),outname), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-12T04:54:45.425042Z",
     "start_time": "2020-01-12T04:54:45.329582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JP Morgan</th>\n",
       "      <th>Etoro</th>\n",
       "      <th>Google</th>\n",
       "      <th>Visa</th>\n",
       "      <th>Goldman Sachs</th>\n",
       "      <th>Unilever</th>\n",
       "      <th>Deloitte</th>\n",
       "      <th>Samsung</th>\n",
       "      <th>Wells Fargo</th>\n",
       "      <th>Allianz</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Commerzbank</th>\n",
       "      <th>tweets</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this looks like it might be a good google anal...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>like that most google apps support multiple ac...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how to track unclicked video ad impressions wi...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>using google analytics to measure social media...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>business analytics &amp;amp; reporting #manager (v...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@eric_analytics porsche is the perfect example...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>it's lunch time during our @googleanalytics tr...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>el usuario de google analytics #infografia #in...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>google analytics is an incredibly important to...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how to discover analytics with the google+ das...</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       JP Morgan  Etoro  Google  Visa  Goldman Sachs  Unilever  Deloitte  \\\n",
       "0              0      0       1     0              0         0         0   \n",
       "1              0      0       1     0              0         0         0   \n",
       "2              0      0       1     0              0         0         0   \n",
       "3              0      0       1     0              0         0         0   \n",
       "4              1      0       0     0              0         0         0   \n",
       "...          ...    ...     ...   ...            ...       ...       ...   \n",
       "29995          0      0       0     0              0         0         0   \n",
       "29996          0      0       1     0              0         0         0   \n",
       "29997          0      0       1     0              0         0         0   \n",
       "29998          0      0       1     0              0         0         0   \n",
       "29999          0      0       1     0              0         0         0   \n",
       "\n",
       "       Samsung  Wells Fargo  Allianz  Apple  Commerzbank  \\\n",
       "0            0            0        0      0            0   \n",
       "1            0            0        0      0            0   \n",
       "2            0            0        0      0            0   \n",
       "3            0            0        0      0            0   \n",
       "4            0            0        0      0            0   \n",
       "...        ...          ...      ...    ...          ...   \n",
       "29995        0            0        0      1            0   \n",
       "29996        0            0        0      0            0   \n",
       "29997        0            0        0      0            0   \n",
       "29998        0            0        0      0            0   \n",
       "29999        0            0        0      0            0   \n",
       "\n",
       "                                                  tweets  year  \n",
       "0      this looks like it might be a good google anal...  2013  \n",
       "1      like that most google apps support multiple ac...  2013  \n",
       "2      how to track unclicked video ad impressions wi...  2013  \n",
       "3      using google analytics to measure social media...  2013  \n",
       "4      business analytics &amp; reporting #manager (v...  2013  \n",
       "...                                                  ...   ...  \n",
       "29995  @eric_analytics porsche is the perfect example...  2015  \n",
       "29996  it's lunch time during our @googleanalytics tr...  2015  \n",
       "29997  el usuario de google analytics #infografia #in...  2015  \n",
       "29998  google analytics is an incredibly important to...  2015  \n",
       "29999  how to discover analytics with the google+ das...  2015  \n",
       "\n",
       "[30000 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# pd.read_csv('/Users/averysoh/Google Drive (racass1234@gmail.com)/AI Project/Avery_output/AI_sequential_topic/AI_topic_company_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
